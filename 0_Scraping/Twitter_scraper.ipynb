{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a34c4216-632a-4c34-b7f5-c4fe85d0abc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading https://files.pythonhosted.org/packages/10/56/8288d1813a68c1e0638515dbb777fce6d87d0d240e683216f956145310e6/selenium-4.11.2-py3-none-any.whl (7.2MB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (2.0.7)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/dd/b61fa61b186d3267ef3903048fbee29132963ae762fb70b08d4a3cd6f7aa/trio-0.22.2-py3-none-any.whl (400kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.2)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/58/e860788190eba3bcce367f74d29c4675466ce8dddfba85f7827588416f01/wsproto-1.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sniffio in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from attrs>=20.1.0->trio~=0.17->selenium) (4.13.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->attrs>=20.1.0->trio~=0.17->selenium) (3.15.0)\n",
      "Installing collected packages: outcome, sortedcontainers, trio, h11, wsproto, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.11.2 sortedcontainers-2.4.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e3a8018-86cc-4439-b511-31f499c73d5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9; python_version < \"3.11\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: idna in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from attrs>=20.1.0->trio~=0.17->selenium) (4.13.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->attrs>=20.1.0->trio~=0.17->selenium) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->attrs>=20.1.0->trio~=0.17->selenium) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3417bd33-426c-4884-b853-c1b90b1450c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/b5/3bd0b038d80950ec13e6a2c8d03ed8354867dc60064b172f2f4ffac8afbe/webdriver_manager-4.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from webdriver-manager) (24.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from webdriver-manager) (0.21.1)\n",
      "Requirement already satisfied: requests in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from requests->webdriver-manager) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from requests->webdriver-manager) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benni\\twint\\twint_env\\lib\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed9708b3-12f1-4781-8a44-eeec84ff2ba6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenvNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading https://files.pythonhosted.org/packages/64/62/f19d1e9023aacb47241de3ab5a5d5fedf32c78a71a9e365bb2153378c141/python_dotenv-0.21.1-py3-none-any.whl\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 24.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "717f3bc7-3107-4c1c-98c9-cdcde2fc8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Chrome on Windows\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\",\n",
    "\n",
    "        # Chrome on macOS\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "\n",
    "        # Chrome on Linux\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\",\n",
    "\n",
    "        # Firefox on Windows\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "\n",
    "        # Firefox on macOS\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "\n",
    "        # Firefox on Linux\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "\n",
    "        # Safari on macOS\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.2 Safari/605.1.15\",\n",
    "\n",
    "        # Edge on Windows\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.864.59 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.902.55 Safari/537.36 Edg/92.0.902.55\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.961.38 Safari/537.36 Edg/93.0.961.38\",\n",
    "\n",
    "        ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "# Initialize the Chrome WebDriver with configured options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Start maximized (full-screen)\n",
    "chrome_options.add_argument(f\"user-agent={get_random_user_agent()}\")\n",
    "# chrome_options.add_argument(\"--headless\")  # Use headless mode for stealth\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(\"https://x.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2258cb-2bf0-4571-be6d-7407f7e90057",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "driver.get(\"http://twitter.com/gdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c687815-bd79-47dc-9883-2fff955ea374",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text from section 'accessible-list-0':\n",
      "Aravind Srinivas’s posts\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "13h\n",
      "If you’re a Canadian university student and want to get free Perplexity Pro for your campus, email us! \n",
      "Quote\n",
      "VJ\n",
      "@VihaanJagiasi\n",
      "·\n",
      "Nov 17\n",
      "Emailed \n",
      "@AravSrinivas\n",
      " & \n",
      "@ramanrmalik\n",
      " to get perplexity pro free for Waterloo. \n",
      "\n",
      "\n",
      "@UWaterloo\n",
      "  folks - can we make it happen? \n",
      "\n",
      "\n",
      "signup here: \n",
      "http://\n",
      "pplx.ai/ca-student\n",
      "Show more\n",
      "78\n",
      "39\n",
      "573\n",
      "84K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "20h\n",
      "Yes. Stop using Temu. We need something that lets you actually shop like a billionaire.\n",
      "Quote\n",
      "eswarpr\n",
      "@eswarpr\n",
      "·\n",
      "20h\n",
      "Replying to \n",
      "@AravSrinivas\n",
      " \n",
      "and\n",
      " \n",
      "@eshear\n",
      "Actual quality recommendation rather than which merchant bid the most $$$ or the highest price. We are being Temu’d into mediocrity.\n",
      "33\n",
      "13\n",
      "400\n",
      "69K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "20h\n",
      ".\n",
      "@eshear\n",
      " once complained to me that he couldn’t shop for underwears online. Is online shopping really hard for you guys these days? What can improve there?\n",
      "113\n",
      "15\n",
      "610\n",
      "175K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "20h\n",
      "Former CEO of OpenAI\n",
      "8\n",
      "3\n",
      "135\n",
      "13K\n",
      "Who to follow\n",
      "Dustin Tran\n",
      "@dustinvtran\n",
      "Follow\n",
      "Click to Follow dustinvtran\n",
      "Research Scientist at Google DeepMind. I lead evaluation at Gemini / Bard. AI, Bayesian statistics, deep learning.\n",
      "Danijar Hafner\n",
      "@danijarh\n",
      "Follow\n",
      "Click to Follow danijarh\n",
      "Building AI that makes autonomous decisions using world models, artificial curiosity, and temporal abstraction \n",
      "@DeepMind\n",
      "Tim Rocktäschel\n",
      "@_rockt\n",
      "Follow\n",
      "Click to Follow _rockt\n",
      "Director and Open-Endedness Team Lead \n",
      "@GoogleDeepMind\n",
      ", Professor of AI \n",
      "@AI_UCL\n",
      ", PI \n",
      "@UCL_DARK\n",
      ", Fellow \n",
      "@ELLISforEurope\n",
      ".\n",
      "Show more\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "21h\n",
      "48\n",
      "27\n",
      "424\n",
      "90K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 16\n",
      "Thank you, Harj. Saturday morning feedback like this is what pushes the team to keep improving and working hard!\n",
      "Quote\n",
      "Harj Taggar\n",
      "@harjtaggar\n",
      "·\n",
      "Nov 15\n",
      "Some products are so much better than the current way of doing something that you start doing a lot more of the thing.  Uber made me pay people to drive me around a lot more. Now Perplexity makes me ask the Internet a lot more questions than I did before.\n",
      "20\n",
      "7\n",
      "260\n",
      "33K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 16\n",
      "Yep\n",
      "Quote\n",
      "Erik Zahnlecker\n",
      "@ezahnlecker\n",
      "·\n",
      "Nov 16\n",
      "There is no comparison here\n",
      "\n",
      "\n",
      "@perplexity_ai\n",
      " massively out executing \n",
      "$GOOG\n",
      ", a company with 235x the market cap and ~1500x the employee count of Perplexity\n",
      "28\n",
      "15\n",
      "379\n",
      "33K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 16\n",
      "NVIDIA earnings call next week. What do you think is going to happen in the after hours?\n",
      "106\n",
      "33\n",
      "901\n",
      "155K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 16\n",
      "No. No user data is shared between Uber and Perplexity in either direction.\n",
      "Quote\n",
      "Wack\n",
      "@wack700\n",
      "·\n",
      "Nov 14\n",
      "Replying to \n",
      "@sierracatalina1\n",
      " \n",
      "@perplexity_ai\n",
      " \n",
      "and\n",
      " \n",
      "@Polymarket\n",
      "interesting... \n",
      "i think they also partnered with uber to get their data\n",
      "7\n",
      "11\n",
      "180\n",
      "29K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 15\n",
      "The Californian state tax on capital gains is 13.3% on top of the Federal 20%, and an additional 3.8% for Obama Care. Yet, San Francisco Bay Area, which produces a significant chunk of the CA taxes, has hardly changed in years. Let’s hope the new elected representatives do\n",
      "Show more\n",
      "94\n",
      "111\n",
      "1.2K\n",
      "163K\n",
      "Aravind Srinivas\n",
      "@AravSrinivas\n",
      "·\n",
      "Nov 14\n",
      "If we did decide to do it, what do you want to see there for the first version that will make you switch from Chrome? In a way, Chrome is the Great Wall of Google. Once you pass it, mission accomplished.\n",
      "Quote\n",
      "Mannas\n",
      "@Mannas5441\n",
      "·\n",
      "Nov 14\n",
      "Replying to \n",
      "@AravSrinivas\n",
      "again same question, why isnt perplexity making its own browser. Also pls check mail \n",
      "541\n",
      "110\n",
      "1.8K\n",
      "585K\n",
      "username: AravSrinivas\n",
      "pattern: @AravSrinivas\\n·\\n(.*?)\\n([^\\n]+)\n",
      "Extracted Posts Data:\n",
      "   First Line                                        Second Line\n",
      "0         13h  If you’re a Canadian university student and wa...\n",
      "1         20h  Yes. Stop using Temu. We need something that l...\n",
      "2         20h                                                  .\n",
      "3         20h                               Former CEO of OpenAI\n",
      "4         21h                                                 48\n",
      "5      Nov 16  Thank you, Harj. Saturday morning feedback lik...\n",
      "6      Nov 16                                                Yep\n",
      "7      Nov 16  NVIDIA earnings call next week. What do you th...\n",
      "8      Nov 16  No. No user data is shared between Uber and Pe...\n",
      "9      Nov 15  The Californian state tax on capital gains is ...\n",
      "10     Nov 14  If we did decide to do it, what do you want to...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def srape_profile(url):\n",
    "    driver.get(url)\n",
    "    random.uniform(2, 5)\n",
    "    # Step 3: Scroll the page multiple times\n",
    "    scroll_times = 7  # Adjust the number of times you want to scroll\n",
    "    for _ in range(scroll_times):\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "    \n",
    "    time.sleep(random.uniform(2, 5))\n",
    "    # Step 4: Extract the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Step 5: Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \"\"\"\n",
    "    # Step 6: Extract content from the specified section\n",
    "    section = soup.find('section', {'aria-labelledby': 'accessible-list-4'})\n",
    "    if section:\n",
    "        posts = section.get_text(separator=\"\\n\").strip()\n",
    "        print(\"Extracted Text:\")\n",
    "        #print(posts)\n",
    "    else:\n",
    "        print(\"Section not found\")\n",
    "    \"\"\"\n",
    "    # Step 6: Dynamically extract content from sections with 'aria-labelledby' attribute\n",
    "    sections = soup.find_all('section', {'aria-labelledby': True})\n",
    "    \n",
    "    # Loop through all sections to find the one with relevant content\n",
    "    for section in sections:\n",
    "        section_id = section.get('aria-labelledby')\n",
    "        posts = section.get_text(separator=\"\\n\").strip()\n",
    "        \n",
    "        if posts:\n",
    "            print(f\"Extracted Text from section '{section_id}':\")\n",
    "            print(posts)\n",
    "            break  # Stop after finding the first non-empty section\n",
    "    else:\n",
    "        print(\"Relevant section not found\")    \n",
    "    # Step 7: Clean up and close the browser\n",
    "    #driver.quit()\n",
    "    \n",
    "    # Sample text output from the scraping process\n",
    "    scraped_text = posts\n",
    "\n",
    "\n",
    "    #Defining Username:\n",
    "    # Regular expression pattern to extract the username\n",
    "    pattern1 = r\"twitter\\.com/([^/?]+)\"\n",
    "    \n",
    "    # Extract the username\n",
    "    match = re.search(pattern1, url)\n",
    "    if match:\n",
    "        username = match.group(1)\n",
    "        print(\"username:\", username)\n",
    "    else:\n",
    "        print(\"No username found.\")\n",
    "    # Updated Regular Expression to capture multiple entries\n",
    "    pattern = fr\"@{username}\\n·\\n(.*?)\\n([^\\n]+)\"\n",
    "    print(\"pattern:\", pattern)\n",
    "    # Extract matches using the updated regex\n",
    "    matches = re.findall(pattern, scraped_text)\n",
    "    \n",
    "    # Convert the matches into a DataFrame\n",
    "    df = pd.DataFrame(matches, columns=[\"First Line\", \"Second Line\"])\n",
    "    \n",
    "    # Display the resulting DataFrame\n",
    "    print(\"Extracted Posts Data:\")\n",
    "    print(df)\n",
    "\n",
    "srape_profile(\"https://twitter.com/AravSrinivas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f921f8a-ca12-432c-8cf6-ae719c435546",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text from section 'accessible-list-0':\n",
      "Extracted Posts Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>First Line</th>\n",
       "      <th>Second Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>9 Oct</td>\n",
       "      <td>Naming a meeting room after my favorite mathem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>15 Oct</td>\n",
       "      <td>Perplexity Finance:  real time stock prices, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>11 Oct</td>\n",
       "      <td>“Doors that open like this”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>4 Jan</td>\n",
       "      <td>Excited to announce we've raised 73.6M$ at 520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>30 Oct</td>\n",
       "      <td>Modi tweeting GitHub stats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>1 Aug</td>\n",
       "      <td>I am not in Tihar Jail right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>3 Nov</td>\n",
       "      <td>Elon Musk is essential to the future of Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>15 Jul</td>\n",
       "      <td>New Perplexity Office!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>8 Sep 2023</td>\n",
       "      <td>In the arena, trying stuff.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                URL  First Line  \\\n",
       "0  https://twitter.com/AravSrinivas       9 Oct   \n",
       "1  https://twitter.com/AravSrinivas      15 Oct   \n",
       "2  https://twitter.com/AravSrinivas      11 Oct   \n",
       "3  https://twitter.com/AravSrinivas       4 Jan   \n",
       "4  https://twitter.com/AravSrinivas      30 Oct   \n",
       "5  https://twitter.com/AravSrinivas       1 Aug   \n",
       "6  https://twitter.com/AravSrinivas       3 Nov   \n",
       "7  https://twitter.com/AravSrinivas      15 Jul   \n",
       "8  https://twitter.com/AravSrinivas  8 Sep 2023   \n",
       "\n",
       "                                         Second Line  \n",
       "0  Naming a meeting room after my favorite mathem...  \n",
       "1  Perplexity Finance:  real time stock prices, d...  \n",
       "2                        “Doors that open like this”  \n",
       "3  Excited to announce we've raised 73.6M$ at 520...  \n",
       "4                         Modi tweeting GitHub stats  \n",
       "5                   I am not in Tihar Jail right now  \n",
       "6  Elon Musk is essential to the future of Americ...  \n",
       "7                             New Perplexity Office!  \n",
       "8                        In the arena, trying stuff.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "# Step 1: Initialize the browser driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def srape_profile(url):\n",
    "    # Step 2: Open the URL\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    # Step 3: Scroll the page multiple times\n",
    "    scroll_times = 8\n",
    "    for _ in range(scroll_times):\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "    \n",
    "    time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    # Step 4: Extract the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Step 5: Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Step 6: Dynamically extract content from sections with 'aria-labelledby' attribute\n",
    "    sections = soup.find_all('section', {'aria-labelledby': True})\n",
    "    \n",
    "    # Loop through all sections to find the one with relevant content\n",
    "    posts = None\n",
    "    for section in sections:\n",
    "        section_id = section.get('aria-labelledby')\n",
    "        posts = section.get_text(separator=\"\\n\").strip()\n",
    "        if posts:\n",
    "            print(f\"Extracted Text from section '{section_id}':\")\n",
    "            #print(posts)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Relevant section not found\")\n",
    "    \n",
    "    # Step 7: Clean up and close the browser\n",
    "    #driver.quit()\n",
    "    \n",
    "    if not posts:\n",
    "        print(\"No posts found.\")\n",
    "        return\n",
    "    \n",
    "    # Step 8: Extract the username from the URL\n",
    "    pattern1 = r\"twitter\\.com/([^/?]+)\"\n",
    "    match = re.search(pattern1, url)\n",
    "    if match:\n",
    "        username = match.group(1)\n",
    "        #print(\"Username:\", username)\n",
    "    else:\n",
    "        print(\"No username found.\")\n",
    "        return\n",
    "    \n",
    "    # Step 9: Use regular expression to extract posts based on the username\n",
    "    pattern = fr\"@{username}\\n·\\n(.*?)\\n([^\\n]+)\"\n",
    "    #print(\"Pattern:\", pattern)\n",
    "    \n",
    "    # Extract matches using the updated regex\n",
    "    matches = re.findall(pattern, posts)\n",
    "    \n",
    "    # Step 10: Convert the matches into a DataFrame and add the URL as a new column\n",
    "    if matches:\n",
    "        df = pd.DataFrame(matches, columns=[\"First Line\", \"Second Line\"])\n",
    "        df.insert(0, \"URL\", url)  # Insert the URL as the first column\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        df = pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "    \n",
    "    # Step 11: Display the resulting DataFrame\n",
    "    print(\"Extracted Posts Data:\")\n",
    "    #print(df)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = srape_profile(\"https://twitter.com/AravSrinivas\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5afff945-a7c2-4c8a-a801-7fba26f2fc7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2078964429.py, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\benni\\AppData\\Local\\Temp\\ipykernel_30948\\2078964429.py\"\u001b[1;36m, line \u001b[1;32m69\u001b[0m\n\u001b[1;33m    return df\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize the browser driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "def scrape_profile(url):\n",
    "    \"\"\"Scrapes tweets from a given Twitter profile URL.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Open the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Step 3: Scroll the page multiple times\n",
    "        scroll_times = 7\n",
    "        for _ in range(scroll_times):\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Step 4: Extract the page source after scrolling\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Step 5: Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Step 6: Dynamically extract content from sections with 'aria-labelledby' attribute\n",
    "        sections = soup.find_all('section', {'aria-labelledby': True})\n",
    "        posts = None\n",
    "        for section in sections:\n",
    "            posts = section.get_text(separator=\"\\n\").strip()\n",
    "            if posts:\n",
    "                break\n",
    "        \n",
    "        if not posts:\n",
    "            return pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        \n",
    "        # Step 7: Extract the username from the URL\n",
    "        pattern1 = r\"twitter\\.com/([^/?]+)\"\n",
    "        match = re.search(pattern1, url)\n",
    "        if match:\n",
    "            username = match.group(1)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        \n",
    "        # Step 8: Extract tweets using regex pattern\n",
    "        pattern = fr\"@{username}\\n·\\n(.*?)\\n([^\\n]+)\"\n",
    "        matches = re.findall(pattern, posts)\n",
    "        \n",
    "        # Step 9: Convert matches into a DataFrame and add the URL\n",
    "        if matches:\n",
    "            df = pd.DataFrame(matches, columns=[\"First Line\", \"Second Line\"])\n",
    "            df.insert(0, \"URL\", url)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "    \n",
    "    finally:\n",
    "        #driver.quit()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scrape_multiple_profiles(df):\n",
    "    \"\"\"Iterates through a DataFrame column containing Twitter links.\"\"\"\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    all_results = pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "    \n",
    "    # Loop through each row in the input DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['Twitter_Links']\n",
    "        print(f\"Scraping profile: {url}\")\n",
    "        \n",
    "        # Scrape the profile and get the result DataFrame\n",
    "        result_df = scrape_profile(url)\n",
    "        \n",
    "        # Append the results to the combined DataFrame\n",
    "        all_results = pd.concat([all_results, result_df], ignore_index=True)\n",
    "        print(all_results)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Sample DataFrame with Twitter links\n",
    "founder_df = pd.read_csv(r\"C:..\\Open_startups_Titter_LinkedIn.csv\")  # Assuming the column with URLs is named \"LinkedIn_url\"\n",
    "\n",
    "\n",
    "# Call the function to scrape multiple profiles\n",
    "result_df = scrape_multiple_profiles(founder_df)\n",
    "\n",
    "# Display the consolidated results\n",
    "print(\"Final Extracted Data:\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2eac2e-e835-4499-850f-bdb8e351872f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization Name</th>\n",
       "      <th>Linkedin_url</th>\n",
       "      <th>Twitter_Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/thegdb</td>\n",
       "      <td>http://twitter.com/gdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/pamela-vagata-8396074</td>\n",
       "      <td>https://twitter.com/pam_vagata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/vickicheung</td>\n",
       "      <td>http://twitter.com/vmcheung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perplexity;https://www.crunchbase.com/organiza...</td>\n",
       "      <td>https://www.linkedin.com/in/andykon</td>\n",
       "      <td>https://twitter.com/andykonwinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perplexity;https://www.crunchbase.com/organiza...</td>\n",
       "      <td>https://www.linkedin.com/in/aravind-srinivas-1...</td>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>Nanoleaf;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>http://www.linkedin.com/in/gimmychu</td>\n",
       "      <td>https://twitter.com/gimmychu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>Revelate;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>https://www.linkedin.com/in/franciswenzel/</td>\n",
       "      <td>https://twitter.com/wenzelware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Revelate;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>https://www.linkedin.com/in/codingtony/</td>\n",
       "      <td>https://twitter.com/codingtony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>TrustSwap;https://www.crunchbase.com/organizat...</td>\n",
       "      <td>https://www.linkedin.com/in/jeffkirdeikis/</td>\n",
       "      <td>https://twitter.com/JeffKirdeikis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>CareGuide;https://www.crunchbase.com/organizat...</td>\n",
       "      <td>http://www.linkedin.com/in/johngreen</td>\n",
       "      <td>http://twitter.com/johnphilipgreen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1268 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Organization Name  \\\n",
       "0     OpenAI;https://www.crunchbase.com/organization...   \n",
       "1     OpenAI;https://www.crunchbase.com/organization...   \n",
       "2     OpenAI;https://www.crunchbase.com/organization...   \n",
       "3     Perplexity;https://www.crunchbase.com/organiza...   \n",
       "4     Perplexity;https://www.crunchbase.com/organiza...   \n",
       "...                                                 ...   \n",
       "1263  Nanoleaf;https://www.crunchbase.com/organizati...   \n",
       "1264  Revelate;https://www.crunchbase.com/organizati...   \n",
       "1265  Revelate;https://www.crunchbase.com/organizati...   \n",
       "1266  TrustSwap;https://www.crunchbase.com/organizat...   \n",
       "1267  CareGuide;https://www.crunchbase.com/organizat...   \n",
       "\n",
       "                                           Linkedin_url  \\\n",
       "0                    https://www.linkedin.com/in/thegdb   \n",
       "1     https://www.linkedin.com/in/pamela-vagata-8396074   \n",
       "2               https://www.linkedin.com/in/vickicheung   \n",
       "3                   https://www.linkedin.com/in/andykon   \n",
       "4     https://www.linkedin.com/in/aravind-srinivas-1...   \n",
       "...                                                 ...   \n",
       "1263                http://www.linkedin.com/in/gimmychu   \n",
       "1264         https://www.linkedin.com/in/franciswenzel/   \n",
       "1265            https://www.linkedin.com/in/codingtony/   \n",
       "1266         https://www.linkedin.com/in/jeffkirdeikis/   \n",
       "1267               http://www.linkedin.com/in/johngreen   \n",
       "\n",
       "                           Twitter_Links  \n",
       "0                 http://twitter.com/gdb  \n",
       "1         https://twitter.com/pam_vagata  \n",
       "2            http://twitter.com/vmcheung  \n",
       "3      https://twitter.com/andykonwinski  \n",
       "4       https://twitter.com/AravSrinivas  \n",
       "...                                  ...  \n",
       "1263        https://twitter.com/gimmychu  \n",
       "1264      https://twitter.com/wenzelware  \n",
       "1265      https://twitter.com/codingtony  \n",
       "1266   https://twitter.com/JeffKirdeikis  \n",
       "1267  http://twitter.com/johnphilipgreen  \n",
       "\n",
       "[1268 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "founder_df = pd.read_csv(r\"C:..\\Open_startups_Titter_LinkedIn.csv\")  # Assuming the column with URLs is named \"LinkedIn_url\"\n",
    "founder_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b2efee8-01a2-4a91-b62d-8bf4bfbf3cc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file not found. Please check the file path.\n",
      "Scraping profile: https://twitter.com/vishakha041\n",
      "1\n",
      "Scraping profile: http://twitter.com/benkinney\n",
      "2\n",
      "Scraping profile: https://twitter.com/Xperience_Chris\n",
      "3\n",
      "Scraping profile: https://twitter.com/defpan\n",
      "4\n",
      "Scraping profile: https://twitter.com/snehalfulzele\n",
      "5\n",
      "Scraping profile: http://twitter.com/davidcranor\n",
      "6\n",
      "Scraping profile: https://twitter.com/maxlobovsky\n",
      "7\n",
      "Scraping profile: https://twitter.com/linder78\n",
      "8\n",
      "Scraping profile: https://twitter.com/Eorbagheri\n",
      "9\n",
      "Scraping profile: https://twitter.com/FredManby\n",
      "10\n",
      "Scraping profile: https://twitter.com/tfmiller3\n",
      "11\n",
      "Scraping profile: https://twitter.com/mrjustinmac\n",
      "12\n",
      "Scraping profile: http://twitter.com/schuylercbrown\n",
      "13\n",
      "Scraping profile: http://twitter.com/Altimor\n",
      "14\n",
      "Scraping profile: https://twitter.com/sean_donovan\n",
      "15\n",
      "Scraping profile: http://twitter.com/jonomillin\n",
      "16\n",
      "Scraping profile: http://twitter.com/mikewinn\n",
      "17\n",
      "Scraping profile: http://twitter.com/nickponline\n",
      "18\n",
      "Scraping profile: http://twitter.com/Altimor\n",
      "19\n",
      "Scraping profile: https://twitter.com/rahoolsidoo\n",
      "20\n",
      "Scraping profile: https://twitter.com/tradingview\n",
      "21\n",
      "Scraping profile: http://twitter.com/stanbokov\n",
      "22\n",
      "Scraping profile: http://twitter.com/frederickcook\n",
      "23\n",
      "Scraping profile: https://twitter.com/matt_graham_\n",
      "24\n",
      "Scraping profile: https://twitter.com/resetbrian\n",
      "25\n",
      "Scraping profile: https://twitter.com/bowie_c4g\n",
      "26\n",
      "Scraping profile: https://twitter.com/amnigos\n",
      "27\n",
      "Scraping profile: https://twitter.com/shumochu\n",
      "28\n",
      "Scraping profile: https://twitter.com/victorJi15\n",
      "29\n",
      "Scraping profile: https://twitter.com/marcinzukowski\n",
      "30\n",
      "Scraping profile: http://twitter.com/laserlikemike\n",
      "31\n",
      "Scraping profile: https://twitter.com/pwang\n",
      "32\n",
      "Scraping profile: https://twitter.com/teoliphant\n",
      "33\n",
      "Scraping profile: http://twitter.com/BrendanEich\n",
      "34\n",
      "Scraping profile: http://twitter.com/brianbondy\n",
      "35\n",
      "Scraping profile: https://twitter.com/senorarroz\n",
      "36\n",
      "Scraping profile: https://twitter.com/jobertabma\n",
      "37\n",
      "Scraping profile: https://twitter.com/michielprins\n",
      "38\n",
      "Scraping profile: https://www.twitter.com/derekherrera\n",
      "39\n",
      "Scraping profile: https://twitter.com/asmengistu\n",
      "40\n",
      "Scraping profile: https://twitter.com/cturlica\n",
      "41\n",
      "Scraping profile: https://twitter.com/NickAtLoot\n",
      "42\n",
      "Scraping profile: https://twitter.com/ryancohen\n",
      "43\n",
      "Scraping profile: https://twitter.com/mackaygeo\n",
      "44\n",
      "Scraping profile: https://twitter.com/geochurch\n",
      "45\n",
      "Scraping profile: https://twitter.com/JurgiCamblong\n",
      "46\n",
      "Scraping profile: https://twitter.com/larsmsteinmetz\n",
      "47\n",
      "Scraping profile: https://twitter.com/adampnathan\n",
      "48\n",
      "Scraping profile: https://twitter.com/_algoldstein\n",
      "49\n",
      "Scraping profile: https://twitter.com/alexg0\n",
      "50\n",
      "Scraping profile: https://twitter.com/mattpolega\n",
      "51\n",
      "Scraping profile: https://twitter.com/scottecrouch\n",
      "52\n",
      "Scraping profile: http://twitter.com/TitoBGoldstein\n",
      "53\n",
      "Scraping profile: https://twitter.com/davidberry26\n",
      "54\n",
      "Scraping profile: https://twitter.com/NoubarAfeyan\n",
      "55\n",
      "Scraping profile: https://twitter.com/liamdon\n",
      "56\n",
      "Scraping profile: https://twitter.com/samchaudhary_\n",
      "57\n",
      "Scraping profile: http://twitter.com/lindarottenberg\n",
      "58\n",
      "Scraping profile: http://twitter.com/alexkehr\n",
      "59\n",
      "Scraping profile: http://twitter.com/ScienceGilman\n",
      "60\n",
      "Scraping profile: http://twitter.com/mjones\n",
      "61\n",
      "Scraping profile: http://twitter.com/macadaan\n",
      "62\n",
      "Scraping profile: http://twitter.com/peterpham\n",
      "63\n",
      "Scraping profile: http://twitter.com/tomdare\n",
      "64\n",
      "Scraping profile: https://twitter.com/starkrampf\n",
      "65\n",
      "Scraping profile: https://twitter.com/mikeklinker1\n",
      "66\n",
      "Scraping profile: http://twitter.com/AndrewYNg\n",
      "67\n",
      "Scraping profile: https://twitter.com/daphnekoller\n",
      "68\n",
      "Scraping profile: https://twitter.com/spenserskates\n",
      "69\n",
      "Scraping profile: https://twitter.com/maravamudan\n",
      "70\n",
      "Scraping profile: http://twitter.com/danbyday\n",
      "71\n",
      "Scraping profile: http://twitter.com/travisv\n",
      "72\n",
      "Scraping profile: https://twitter.com/steele32\n",
      "73\n",
      "Scraping profile: https://twitter.com/ericwilliamrea\n",
      "74\n",
      "Scraping profile: http://twitter.com/amittakes\n",
      "75\n",
      "Scraping profile: http://twitter.com/bfirsh\n",
      "76\n",
      "Scraping profile: https://twitter.com/purveshkhatri\n",
      "77\n",
      "Scraping profile: https://twitter.com/timsweeney83\n",
      "78\n",
      "Scraping profile: http://twitter.com/malcolmong\n",
      "79\n",
      "Scraping profile: http://twitter.com/austinogilvie\n",
      "80\n",
      "Scraping profile: https://twitter.com/amirhusain_tx\n",
      "81\n",
      "Scraping profile: https://twitter.com/jerryjliu0\n",
      "82\n",
      "Scraping profile: https://twitter.com/disiok\n",
      "83\n",
      "Scraping profile: http://twitter.com/alexhalliday\n",
      "84\n",
      "Scraping profile: https://twitter.com/naveengrao\n",
      "85\n",
      "Scraping profile: https://twitter.com/ericllam\n",
      "86\n",
      "Scraping profile: https://twitter.com/davidarfin\n",
      "87\n",
      "Scraping profile: https://twitter.com/KyleJamesKirwan\n",
      "88\n",
      "Scraping profile: https://twitter.com/andyseavers\n",
      "89\n",
      "Scraping profile: http://twitter.com/ArthurLozinski\n",
      "90\n",
      "Scraping profile: http://twitter.com/Ramin138\n",
      "91\n",
      "Scraping profile: https://twitter.com/trentseed\n",
      "92\n",
      "Scraping profile: https://mobile.twitter.com/joannastrober\n",
      "93\n",
      "Scraping profile: https://twitter.com/sharonmeers\n",
      "94\n",
      "Scraping profile: https://twitter.com/YablonLaw\n",
      "95\n",
      "Scraping profile: http://twitter.com/jyotibansalsf\n",
      "96\n",
      "Scraping profile: https://twitter.com/SanjayNSF\n",
      "97\n",
      "Scraping profile: http://www.twitter.com/benlamm\n",
      "98\n",
      "Scraping profile: https://www.twitter.com/jacobsboudreau\n",
      "99\n",
      "Scraping profile: http://twitter.com/seanhenryATL\n",
      "100\n",
      "Scraping profile: http://twitter.com/KiteVC\n",
      "101\n",
      "Scraping profile: https://twitter.com/yoshikawahiro\n",
      "102\n",
      "Scraping profile: http://twitter.com/kzk_mover\n",
      "103\n",
      "Scraping profile: https://twitter.com/naftaliharris\n",
      "104\n",
      "Scraping profile: https://twitter.com/drorshimoni\n",
      "105\n",
      "Scraping profile: https://twitter.com/naama_cohen\n",
      "106\n",
      "Scraping profile: https://twitter.com/ozalon_\n",
      "107\n",
      "Scraping profile: https://mobile.twitter.com/shadiahs\n",
      "108\n",
      "Scraping profile: https://twitter.com/joshgeleris\n",
      "109\n",
      "Scraping profile: https://twitter.com/JeniferSnyder\n",
      "110\n",
      "Scraping profile: https://twitter.com/SadeeGamhewa\n",
      "111\n",
      "Scraping profile: https://twitter.com/linder78\n",
      "112\n",
      "Scraping profile: https://twitter.com/kubat\n",
      "113\n",
      "Scraping profile: https://twitter.com/ArtAgrawal\n",
      "114\n",
      "Scraping profile: http://twitter.com/hussein_fazal\n",
      "115\n",
      "Scraping profile: https://twitter.com/mfk\n",
      "116\n",
      "Scraping profile: https://twitter.com/sbyrnes\n",
      "117\n",
      "Scraping profile: https://twitter.com/arjunsprakash\n",
      "118\n",
      "Scraping profile: https://twitter.com/danwrightSF\n",
      "119\n",
      "Scraping profile: http://twitter.com/lg, http://twitter.com/lg\n",
      "120\n",
      "Scraping profile: https://twitter.com/rickdoblin\n",
      "121\n",
      "Scraping profile: https://twitter.com/ekwking\n",
      "122\n",
      "Scraping profile: http://twitter.com/jalehr\n",
      "123\n",
      "Scraping profile: https://twitter.com/nikhil_mathew\n",
      "124\n",
      "Scraping profile: https://twitter.com/malling\n",
      "125\n",
      "Scraping profile: https://twitter.com/mattckennedy\n",
      "126\n",
      "Scraping profile: https://twitter.com/crothe\n",
      "127\n",
      "Scraping profile: https://twitter.com/kwm\n",
      "128\n",
      "Scraping profile: https://twitter.com/adammcarrigan\n",
      "129\n",
      "Scraping profile: http://twitter.com/alibhamed\n",
      "130\n",
      "Scraping profile: http://twitter.com/dannykwells\n",
      "131\n",
      "Scraping profile: https://twitter.com/jkessl178\n",
      "132\n",
      "Scraping profile: http://twitter.com/ryanglasgow\n",
      "133\n",
      "Scraping profile: https://twitter.com/jalalnasir\n",
      "134\n",
      "Scraping profile: http://twitter.com/venedan\n",
      "135\n",
      "Scraping profile: http://twitter.com/dsaezgil\n",
      "136\n",
      "Scraping profile: https://twitter.com/tomasaftalion\n",
      "137\n",
      "Scraping profile: http://twitter.com/shlykur\n",
      "138\n",
      "Scraping profile: https://twitter.com/jakecooney\n",
      "139\n",
      "Scraping profile: http://twitter.com/mattehrlichman\n",
      "140\n",
      "Scraping profile: http://twitter.com/rcastro831\n",
      "141\n",
      "Scraping profile: http://twitter.com/svaustin\n",
      "142\n",
      "Scraping profile: http://www.twitter.com/nickfrosst\n",
      "143\n",
      "Scraping profile: http://twitter.com/cloudchange\n",
      "144\n",
      "Scraping profile: http://twitter.com/eugened\n",
      "145\n",
      "Scraping profile: http://twitter.com/kennethloi\n",
      "146\n",
      "Scraping profile: http://twitter.com/davehariri\n",
      "147\n",
      "Scraping profile: http://twitter.com/mimurchison\n",
      "148\n",
      "Scraping profile: https://twitter.com/mserbinis\n",
      "149\n",
      "Scraping profile: http://twitter.com/adam3us\n",
      "150\n",
      "Scraping profile: https://twitter.com/alexanderfowler\n",
      "151\n",
      "Scraping profile: http://twitter.com/austinhill\n",
      "152\n",
      "Scraping profile: http://twitter.com/e_svenson\n",
      "153\n",
      "Scraping profile: https://twitter.com/jwilkins\n",
      "154\n",
      "Scraping profile: http://twitter.com/MarkFriedenbach\n",
      "155\n",
      "Scraping profile: http://twitter.com/TheBlueMatt\n",
      "156\n",
      "Scraping profile: https://twitter.com/pwuille\n",
      "157\n",
      "Scraping profile: https://twitter.com/jeffcanadamson?lang=en\n",
      "158\n",
      "Scraping profile: https://twitter.com/chrispavlovski\n",
      "159\n",
      "Scraping profile: https://twitter.com/NoahBuchman\n",
      "160\n",
      "Scraping profile: https://twitter.com/apenwarr\n",
      "161\n",
      "Scraping profile: https://twitter.com/davidcrawshaw\n",
      "162\n",
      "Scraping profile: https://twitter.com/danebs\n",
      "163\n",
      "Scraping profile: http://twitter.com/jonathanbixby\n",
      "164\n",
      "Scraping profile: https://twitter.com/joshuabixby\n",
      "165\n",
      "Scraping profile: http://twitter.com/solonang\n",
      "166\n",
      "Scraping profile: https://twitter.com/mitrymin\n",
      "167\n",
      "Scraping profile: https://twitter.com/sbishayiris?lang=en\n",
      "168\n",
      "Scraping profile: https://twitter.com/dtuer\n",
      "169\n",
      "Scraping profile: https://twitter.com/TateHackert\n",
      "170\n",
      "Scraping profile: https://twitter.com/SullyOmarr\n",
      "171\n",
      "Scraping profile: http://twitter.com/fzeisler\n",
      "172\n",
      "Scraping profile: http://twitter.com/getjobber\n",
      "173\n",
      "Scraping profile: https://twitter.com/abidvirani\n",
      "174\n",
      "Scraping profile: https://twitter.com/trentmell\n",
      "175\n",
      "Scraping profile: https://twitter.com/_ehsanmokhtari\n",
      "176\n",
      "Scraping profile: https://twitter.com/MizrahiEtai\n",
      "177\n",
      "Scraping profile: https://www.twitter.com/uoftcompsci\n",
      "178\n",
      "Scraping profile: https://twitter.com/joshuaandrews?lang=en\n",
      "179\n",
      "Scraping profile: http://twitter.com/stephenufford\n",
      "180\n",
      "Scraping profile: http://twitter.com/TanisJorge\n",
      "181\n",
      "Scraping profile: https://www.twitter.com/kordestanchi\n",
      "182\n",
      "Scraping profile: http://twitter.com/ycwest\n",
      "183\n",
      "Scraping profile: https://twitter.com/_Aali\n",
      "184\n",
      "Scraping profile: https://twitter.com/jaygiraud\n",
      "185\n",
      "Scraping profile: http://twitter.com/onemorejsmith\n",
      "186\n",
      "Scraping profile: https://twitter.com/ajcronk\n",
      "187\n",
      "Scraping profile: http://twitter.com/lorientree\n",
      "188\n",
      "Scraping profile: https://twitter.com/rfunduk\n",
      "189\n",
      "Scraping profile: https://twitter.com/kang_simran_\n",
      "190\n",
      "Scraping profile: https://twitter.com/katyasht\n",
      "191\n",
      "Scraping profile: https://twitter.com/ryan_marien\n",
      "192\n",
      "Scraping profile: https://twitter.com/shizaoki\n",
      "193\n",
      "Scraping profile: https://twitter.com/professor_ajay\n",
      "194\n",
      "Scraping profile: https://twitter.com/suzannegildert\n",
      "195\n",
      "Scraping profile: https://twitter.com/ATrotmanGrant\n",
      "196\n",
      "Scraping profile: https://twitter.com/DeanSutton\n",
      "197\n",
      "Scraping profile: https://twitter.com/sinkpoint\n",
      "198\n",
      "Scraping profile: https://twitter.com/Liran_Belenzon\n",
      "199\n",
      "Scraping profile: https://twitter.com/TomLeung_PhD\n",
      "200\n",
      "Scraping profile: https://twitter.com/bryncdn\n",
      "201\n",
      "Scraping profile: https://twitter.com/jonathanamendes\n",
      "202\n",
      "Scraping profile: https://twitter.com/lukeswanek\n",
      "203\n",
      "Scraping profile: http://twitter.com/gapcm\n",
      "204\n",
      "Scraping profile: http://twitter.com/jodyglidden\n",
      "205\n",
      "Scraping profile: https://twitter.com/ygleboeuf\n",
      "206\n",
      "Scraping profile: https://twitter.com/cedricprovost\n",
      "207\n",
      "Scraping profile: http://twitter.com/devongall\n",
      "208\n",
      "Scraping profile: http://twitter.com/michaellitt\n",
      "209\n",
      "Scraping profile: https://twitter.com/quantumstef\n",
      "210\n",
      "Scraping profile: http://twitter.com/touchbistro\n",
      "211\n",
      "Scraping profile: https://twitter.com/BeauCrabill\n",
      "212\n",
      "Scraping profile: https://twitter.com/andrewmcleodiii\n",
      "213\n",
      "Scraping profile: https://twitter.com/owenmadrick\n",
      "214\n",
      "Scraping profile: http://twitter.com/adamjsaint\n",
      "215\n",
      "Scraping profile: http://twitter.com/ianwcrosby\n",
      "216\n",
      "Scraping profile: http://twitter.com/JordanMenashy\n",
      "217\n",
      "Scraping profile: http://twitter.com/rodionovp\n",
      "218\n",
      "Scraping profile: https://twitter.com/erin_stephenson\n",
      "219\n",
      "Scraping profile: https://twitter.com/TimForestell\n",
      "220\n",
      "Scraping profile: https://twitter.com/JoshuaOstrega\n",
      "221\n",
      "Scraping profile: https://twitter.com/professor_ajay\n",
      "222\n",
      "Scraping profile: https://twitter.com/TonyLacavera\n",
      "223\n",
      "Scraping profile: http://twitter.com/evnhu\n",
      "224\n",
      "Scraping profile: https://twitter.com/MillingtonDrew\n",
      "225\n",
      "Scraping profile: https://twitter.com/MattLoszak\n",
      "226\n",
      "Scraping profile: https://twitter.com/jwscycle\n",
      "227\n",
      "Scraping profile: https://twitter.com/rebeccashort2\n",
      "228\n",
      "Scraping profile: https://twitter.com/adamreeds\n",
      "229\n",
      "Scraping profile: https://twitter.com/cryptonomista\n",
      "230\n",
      "Scraping profile: https://twitter.com/rossandrew\n",
      "231\n",
      "Scraping profile: https://twitter.com/chriddyp\n",
      "232\n",
      "Scraping profile: https://twitter.com/plotlygraphs\n",
      "233\n",
      "Scraping profile: https://twitter.com/mfasko\n",
      "234\n",
      "Scraping profile: http://twitter.com/robimbeault\n",
      "235\n",
      "Scraping profile: https://twitter.com/bdyment\n",
      "236\n",
      "Scraping profile: http://twitter.com/Braatzy\n",
      "237\n",
      "Scraping profile: http://twitter.com/ildarshar\n",
      "238\n",
      "Scraping profile: https://twitter.com/vitalypecherski\n",
      "239\n",
      "Scraping profile: http://twitter.com/malayeri\n",
      "240\n",
      "Scraping profile: http://twitter.com/siavash\n",
      "241\n",
      "Scraping profile: https://twitter.com/soh3il\n",
      "242\n",
      "Scraping profile: https://twitter.com/Derek_Vogt\n",
      "243\n",
      "Scraping profile: http://twitter.com/raymondreddy\n",
      "244\n",
      "Scraping profile: https://twitter.com/atkabai\n",
      "245\n",
      "Scraping profile: https://twitter.com/AndreasVaz\n",
      "246\n",
      "Scraping profile: https://twitter.com/aiouy\n",
      "247\n",
      "Scraping profile: https://twitter.com/roybrey\n",
      "248\n",
      "Scraping profile: http://twitter.com/aydin\n",
      "249\n",
      "Scraping profile: http://twitter.com/parkerewb\n",
      "250\n",
      "Scraping profile: https://twitter.com/seashellski\n",
      "251\n",
      "Scraping profile: https://twitter.com/mikepotter\n",
      "252\n",
      "Scraping profile: https://twitter.com/JimVaio\n",
      "253\n",
      "Scraping profile: https://twitter.com/helenissocial\n",
      "254\n",
      "Scraping profile: https://twitter.com/ppoulidis\n",
      "255\n",
      "Scraping profile: https://twitter.com/akhilguptak\n",
      "256\n",
      "Scraping profile: https://twitter.com/cadearsley\n",
      "257\n",
      "Scraping profile: http://twitter.com/fungmoney\n",
      "258\n",
      "Scraping profile: https://twitter.com/jonsimpson\n",
      "259\n",
      "Scraping profile: https://twitter.com/helsontaveras\n",
      "260\n",
      "Scraping profile: https://twitter.com/thetakach\n",
      "261\n",
      "Scraping profile: https://twitter.com/nickysenyard\n",
      "262\n",
      "Scraping profile: https://twitter.com/maximedroux?lang=en\n",
      "263\n",
      "Scraping profile: https://twitter.com/ben_nashman\n",
      "264\n",
      "Scraping profile: http://twitter.com/aliasaria\n",
      "265\n",
      "Scraping profile: https://twitter.com/coryjanssen\n",
      "266\n",
      "Scraping profile: https://twitter.com/Chase_Belair\n",
      "267\n",
      "Scraping profile: http://twitter.com/kerramilli\n",
      "268\n",
      "Scraping profile: https://twitter.com/janschwarz\n",
      "269\n",
      "Scraping profile: https://twitter.com/laurenlake1\n",
      "270\n",
      "Final Extracted Data:\n",
      "                                  URL    First Line  \\\n",
      "0     https://twitter.com/vishakha041        Oct 14   \n",
      "1     https://twitter.com/vishakha041        Oct 14   \n",
      "2     https://twitter.com/vishakha041        Oct 10   \n",
      "3        http://twitter.com/benkinney  Mar 23, 2014   \n",
      "4        http://twitter.com/benkinney  Mar 18, 2014   \n",
      "...                               ...           ...   \n",
      "1304   https://twitter.com/jonsimpson  Nov 22, 2007   \n",
      "1305   https://twitter.com/jonsimpson  Nov 21, 2007   \n",
      "1306   https://twitter.com/jonsimpson  Nov 20, 2007   \n",
      "1307   https://twitter.com/jonsimpson  Nov 20, 2007   \n",
      "1308   https://twitter.com/jonsimpson  Nov 17, 2007   \n",
      "\n",
      "                                            Second Line  \n",
      "0                            ApertureDB Cloud is live!   \n",
      "1                   Yet another great partnership with   \n",
      "2      Thanks for the on point write up on our work at   \n",
      "3     The whole mountain fell down in Oso where I wa...  \n",
      "4     Most people are not willing to sacrifice who t...  \n",
      "...                                                 ...  \n",
      "1304  happening to be in America for thanksgiving: b...  \n",
      "1305  out for Indian buffet lunch, hopefully less dr...  \n",
      "1306       working on the SIGCSE 2008 paper final draft  \n",
      "1307  enjoying some pre-christmas snow in Boston, th...  \n",
      "1308  sitting, wondering what to do on a sunny Bosto...  \n",
      "\n",
      "[1309 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize the browser driver globally with incognito mode\n",
    "\"\"\"chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\"\"\"\n",
    "\n",
    "def scrape_profile(url):\n",
    "    \"\"\"Scrapes tweets from a given Twitter profile URL.\"\"\"\n",
    "    try:\n",
    "        # Step 2: Open the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Step 3: Scroll the page multiple times\n",
    "        scroll_times = 12\n",
    "        for _ in range(scroll_times):\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            driver.execute_script(\"window.scrollBy(0, window.innerHeight);\")\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Step 4: Extract the page source after scrolling\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Step 5: Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Step 6: Dynamically extract content from sections with 'aria-labelledby' attribute\n",
    "        sections = soup.find_all('section', {'aria-labelledby': True})\n",
    "        posts = None\n",
    "        for section in sections:\n",
    "            posts = section.get_text(separator=\"\\n\").strip()\n",
    "            if posts:\n",
    "                break\n",
    "        \n",
    "        # If no posts found, return an empty DataFrame\n",
    "        if not posts:\n",
    "            return pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        \n",
    "        # Step 7: Extract the username from the URL\n",
    "        pattern1 = r\"twitter\\.com/([^/?]+)\"\n",
    "        match = re.search(pattern1, url)\n",
    "        if match:\n",
    "            username = match.group(1)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        \n",
    "        # Step 8: Extract tweets using regex pattern\n",
    "        pattern = fr\"@{username}\\n·\\n(.*?)\\n([^\\n]+)\"\n",
    "        matches = re.findall(pattern, posts)\n",
    "        \n",
    "        # Step 9: Convert matches into a DataFrame and add the URL\n",
    "        if matches:\n",
    "            df = pd.DataFrame(matches, columns=[\"First Line\", \"Second Line\"])\n",
    "            df.insert(0, \"URL\", url)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        \n",
    "        return df  # Correctly indented return statement\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        founder_df = pd.read_csv(r\"C:..\\closed_founder_twitter.csv\")\n",
    "        input_df = founder_df[2000:2500]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: CSV file not found. Please check the file path.\")\n",
    "        founder_df = pd.DataFrame()\n",
    "\n",
    "    # Check if the DataFrame has the required column\n",
    "    if 'Twitter_Links' in input_df.columns:\n",
    "        # Initialize an empty DataFrame to store results\n",
    "        all_results = pd.DataFrame(columns=[\"URL\", \"First Line\", \"Second Line\"])\n",
    "        i=0\n",
    "        # Loop through each row in the input DataFrame\n",
    "        for index, row in input_df.iterrows():\n",
    "            url = row['Twitter_Links']\n",
    "            print(f\"Scraping profile: {url}\")\n",
    "            \n",
    "            # Scrape the profile and get the result DataFrame\n",
    "            result_df = scrape_profile(url)\n",
    "            \n",
    "            # Append the results to the combined DataFrame\n",
    "            all_results = pd.concat([all_results, result_df], ignore_index=True)\n",
    "            #print(all_results)\n",
    "            i=i+1\n",
    "            print(i)\n",
    "            all_results.to_csv(r\"C:..\\Tweets\\Closed_startups_Titter_posts2000to2500.csv\")\n",
    "            \n",
    "        # Display the consolidated results\n",
    "        print(\"Final Extracted Data:\")\n",
    "        print(all_results)\n",
    "    else:\n",
    "        print(\"Error: Column 'Twitter_Links' not found in the CSV file.\")\n",
    "    \n",
    "    # Close the browser once done\n",
    "    #driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ba88ea3-9d8e-4805-84be-0a89240fb2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>First Line</th>\n",
       "      <th>Second Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/vishakha041</td>\n",
       "      <td>Oct 14</td>\n",
       "      <td>Yet another great partnership with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/vishakha041</td>\n",
       "      <td>Oct 10</td>\n",
       "      <td>Thanks for the on point write up on our work at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://twitter.com/benkinney</td>\n",
       "      <td>Mar 23, 2014</td>\n",
       "      <td>The whole mountain fell down in Oso where I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://twitter.com/benkinney</td>\n",
       "      <td>Mar 18, 2014</td>\n",
       "      <td>Most people are not willing to sacrifice who t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitter.com/benkinney</td>\n",
       "      <td>Mar 2, 2014</td>\n",
       "      <td>Boom!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>https://twitter.com/janschwarz</td>\n",
       "      <td>Oct 28, 2011</td>\n",
       "      <td>http://</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>https://twitter.com/janschwarz</td>\n",
       "      <td>Oct 4, 2011</td>\n",
       "      <td>We’re excited to attend our first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>https://twitter.com/janschwarz</td>\n",
       "      <td>Jan 8, 2009</td>\n",
       "      <td>At CES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>https://twitter.com/laurenlake1</td>\n",
       "      <td>Jan 7, 2015</td>\n",
       "      <td>Proud to see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>https://twitter.com/laurenlake1</td>\n",
       "      <td>Nov 27, 2014</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1348 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  URL    First Line  \\\n",
       "0     https://twitter.com/vishakha041        Oct 14   \n",
       "1     https://twitter.com/vishakha041        Oct 10   \n",
       "2        http://twitter.com/benkinney  Mar 23, 2014   \n",
       "3        http://twitter.com/benkinney  Mar 18, 2014   \n",
       "4        http://twitter.com/benkinney   Mar 2, 2014   \n",
       "...                               ...           ...   \n",
       "1343   https://twitter.com/janschwarz  Oct 28, 2011   \n",
       "1344   https://twitter.com/janschwarz   Oct 4, 2011   \n",
       "1345   https://twitter.com/janschwarz   Jan 8, 2009   \n",
       "1346  https://twitter.com/laurenlake1   Jan 7, 2015   \n",
       "1347  https://twitter.com/laurenlake1  Nov 27, 2014   \n",
       "\n",
       "                                            Second Line  \n",
       "0                   Yet another great partnership with   \n",
       "1      Thanks for the on point write up on our work at   \n",
       "2     The whole mountain fell down in Oso where I wa...  \n",
       "3     Most people are not willing to sacrifice who t...  \n",
       "4                                                 Boom!  \n",
       "...                                                 ...  \n",
       "1343                                            http://  \n",
       "1344                 We’re excited to attend our first   \n",
       "1345                                             At CES  \n",
       "1346                                      Proud to see   \n",
       "1347                                                  .  \n",
       "\n",
       "[1348 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7be653b-d434-443b-9c6c-0d387e36a81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Organization Name</th>\n",
       "      <th>Linkedin_url</th>\n",
       "      <th>Twitter_Links</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>Bridgit;https://www.crunchbase.com/organizatio...</td>\n",
       "      <td>https://www.linkedin.com/in/lauren-lake-74832748</td>\n",
       "      <td>https://twitter.com/laurenlake1</td>\n",
       "      <td>lauren-lake-74832748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                  Organization Name  \\\n",
       "999         999  Bridgit;https://www.crunchbase.com/organizatio...   \n",
       "\n",
       "                                         Linkedin_url  \\\n",
       "999  https://www.linkedin.com/in/lauren-lake-74832748   \n",
       "\n",
       "                       Twitter_Links              Username  \n",
       "999  https://twitter.com/laurenlake1  lauren-lake-74832748  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check until where the code scraped-> sometimes scraping code breaks\n",
    "founder_df = pd.read_csv(r\"C:..\\Open_startups_Titter_LinkedIn.csv\")\n",
    "founder_df[founder_df[\"Twitter_Links\"]==\"https://twitter.com/laurenlake1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0c98f1f-5d5e-44d1-a865-85e032411438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Organization Name</th>\n",
       "      <th>Linkedin_url</th>\n",
       "      <th>Twitter_Links</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/thegdb</td>\n",
       "      <td>http://twitter.com/gdb</td>\n",
       "      <td>thegdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/pamela-vagata-8396074</td>\n",
       "      <td>https://twitter.com/pam_vagata</td>\n",
       "      <td>pamela-vagata-8396074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>OpenAI;https://www.crunchbase.com/organization...</td>\n",
       "      <td>https://www.linkedin.com/in/vickicheung</td>\n",
       "      <td>http://twitter.com/vmcheung</td>\n",
       "      <td>vickicheung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Perplexity;https://www.crunchbase.com/organiza...</td>\n",
       "      <td>https://www.linkedin.com/in/andykon</td>\n",
       "      <td>https://twitter.com/andykonwinski</td>\n",
       "      <td>andykon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Perplexity;https://www.crunchbase.com/organiza...</td>\n",
       "      <td>https://www.linkedin.com/in/aravind-srinivas-1...</td>\n",
       "      <td>https://twitter.com/AravSrinivas</td>\n",
       "      <td>aravind-srinivas-16051987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1263</td>\n",
       "      <td>Nanoleaf;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>http://www.linkedin.com/in/gimmychu</td>\n",
       "      <td>https://twitter.com/gimmychu</td>\n",
       "      <td>gimmychu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>1264</td>\n",
       "      <td>Revelate;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>https://www.linkedin.com/in/franciswenzel/</td>\n",
       "      <td>https://twitter.com/wenzelware</td>\n",
       "      <td>franciswenzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>1265</td>\n",
       "      <td>Revelate;https://www.crunchbase.com/organizati...</td>\n",
       "      <td>https://www.linkedin.com/in/codingtony/</td>\n",
       "      <td>https://twitter.com/codingtony</td>\n",
       "      <td>codingtony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1266</td>\n",
       "      <td>TrustSwap;https://www.crunchbase.com/organizat...</td>\n",
       "      <td>https://www.linkedin.com/in/jeffkirdeikis/</td>\n",
       "      <td>https://twitter.com/JeffKirdeikis</td>\n",
       "      <td>jeffkirdeikis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1267</td>\n",
       "      <td>CareGuide;https://www.crunchbase.com/organizat...</td>\n",
       "      <td>http://www.linkedin.com/in/johngreen</td>\n",
       "      <td>http://twitter.com/johnphilipgreen</td>\n",
       "      <td>johngreen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1268 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                  Organization Name  \\\n",
       "0              0  OpenAI;https://www.crunchbase.com/organization...   \n",
       "1              1  OpenAI;https://www.crunchbase.com/organization...   \n",
       "2              2  OpenAI;https://www.crunchbase.com/organization...   \n",
       "3              3  Perplexity;https://www.crunchbase.com/organiza...   \n",
       "4              4  Perplexity;https://www.crunchbase.com/organiza...   \n",
       "...          ...                                                ...   \n",
       "1263        1263  Nanoleaf;https://www.crunchbase.com/organizati...   \n",
       "1264        1264  Revelate;https://www.crunchbase.com/organizati...   \n",
       "1265        1265  Revelate;https://www.crunchbase.com/organizati...   \n",
       "1266        1266  TrustSwap;https://www.crunchbase.com/organizat...   \n",
       "1267        1267  CareGuide;https://www.crunchbase.com/organizat...   \n",
       "\n",
       "                                           Linkedin_url  \\\n",
       "0                    https://www.linkedin.com/in/thegdb   \n",
       "1     https://www.linkedin.com/in/pamela-vagata-8396074   \n",
       "2               https://www.linkedin.com/in/vickicheung   \n",
       "3                   https://www.linkedin.com/in/andykon   \n",
       "4     https://www.linkedin.com/in/aravind-srinivas-1...   \n",
       "...                                                 ...   \n",
       "1263                http://www.linkedin.com/in/gimmychu   \n",
       "1264         https://www.linkedin.com/in/franciswenzel/   \n",
       "1265            https://www.linkedin.com/in/codingtony/   \n",
       "1266         https://www.linkedin.com/in/jeffkirdeikis/   \n",
       "1267               http://www.linkedin.com/in/johngreen   \n",
       "\n",
       "                           Twitter_Links                   Username  \n",
       "0                 http://twitter.com/gdb                     thegdb  \n",
       "1         https://twitter.com/pam_vagata      pamela-vagata-8396074  \n",
       "2            http://twitter.com/vmcheung                vickicheung  \n",
       "3      https://twitter.com/andykonwinski                    andykon  \n",
       "4       https://twitter.com/AravSrinivas  aravind-srinivas-16051987  \n",
       "...                                  ...                        ...  \n",
       "1263        https://twitter.com/gimmychu                   gimmychu  \n",
       "1264      https://twitter.com/wenzelware              franciswenzel  \n",
       "1265      https://twitter.com/codingtony                 codingtony  \n",
       "1266   https://twitter.com/JeffKirdeikis              jeffkirdeikis  \n",
       "1267  http://twitter.com/johnphilipgreen                  johngreen  \n",
       "\n",
       "[1268 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "founder_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dcf7b-2242-4e9f-9ea8-e402bbe4b765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (twint_env)",
   "language": "python",
   "name": "twint_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
