{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b490649-13ab-4808-9a35-8ed72119c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e36352-64d2-4c2c-b738-95c0734d3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ImageHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96c6d55-ec10-4ae5-87e2-440ea0ba38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyautogui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5e36f75-8212-47a1-a86e-1a6b3c1ed363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import imagehash\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38c984a8-e5bb-4cf8-afae-50fe0f047a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrollAndLoad(driver):\n",
    "    # Function to scroll down the page\n",
    "    def scroll_to_load_all_elements():\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down to the bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            # Wait for new content to load\n",
    "            time.sleep(2)  # Adjust the sleep time as needed\n",
    "            \n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break  # No more new content\n",
    "            last_height = new_height\n",
    "    \n",
    "    # Call the function to scroll down\n",
    "    scroll_to_load_all_elements()\n",
    "    \n",
    "    # Now you can find the <ul> element\n",
    "    #ul_element = driver.find_element(By.CSS_SELECTOR, \"ul.display-flex.list-style-none.flex-wrap\")\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4f80e8-9862-48bc-89d6-8d21ade105d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScrollAndLoad()\n",
    "def linkedInProfiles(driver):\n",
    "    # Find the parent <ul> element by its class name\n",
    "    ul_element = driver.find_element(By.CSS_SELECTOR, \"ul.display-flex.list-style-none.flex-wrap\")\n",
    "\n",
    "    # Find all <li> elements within the <ul>\n",
    "    li_elements = ul_element.find_elements(By.CSS_SELECTOR, \"li.grid.grid__col--lg-8.block.org-people-profile-card__profile-card-spacing\")\n",
    "\n",
    "    \"\"\"Non_existent_linkedIn_Profiles = []\n",
    "    Non_existent_linkedIn_and_no_Image = []\n",
    "    LinkedIn_Url = []\n",
    "    \n",
    "    for i, li in enumerate(li_elements, start=1):  # Start counting from 1 for better readability\n",
    "        # Check for the <a> element with the specified class name\n",
    "        try:\n",
    "            link_element = li.find_element(By.CSS_SELECTOR, \"a.app-aware-link.link-without-visited-state\")\n",
    "            link_url = link_element.get_attribute(\"href\")  # Get the link URL\n",
    "            LinkedIn_Url.append(link_url)\n",
    "\n",
    "        except Exception:\n",
    "            # If the <a> element does not exist, note the position\n",
    "            Non_existent_linkedIn_Profiles.append(i)\n",
    "            try:\n",
    "                # Check for the image element\n",
    "                missing_image = li.find_element(By.CSS_SELECTOR, \"img.evi-image.lazy-image.ghost-person.ember-view\")\n",
    "                Non_existent_linkedIn_and_no_Image.append(i)\n",
    "            except Exception:\n",
    "                # No action needed if the image is not found, since it's already captured in the list.\n",
    "                pass\n",
    "\n",
    "    #Determeing the index of the list of pictures, where the non-available pictures are not included\n",
    "    for num in Non_existent_linkedIn_and_no_Image:\n",
    "        if num in Non_existent_linkedIn_Profiles:\n",
    "            Non_existent_linkedIn_Profiles.remove(num)  # Remove the number from the first list\n",
    "            # Decrement numbers larger than the removed number\n",
    "            Non_existent_linkedIn_Profiles = [x - 1 if x > num else x for x in Non_existent_linkedIn_Profiles]\n",
    "    \"\"\"\n",
    "    Non_existent_linkedIn_Profiles = []\n",
    "    Non_existent_Image = []\n",
    "    LinkedIn_Url = []\n",
    "    memory = []\n",
    "    #print(enumerate(li_elements))\n",
    "    for i, li in enumerate(li_elements, start=1):  # Start counting from 1 for better readability\n",
    "        # Check for the <a> element with the specified class name\n",
    "        try:\n",
    "            link_element = li.find_element(By.CSS_SELECTOR, \"a.app-aware-link.link-without-visited-state\")\n",
    "            link_url = link_element.get_attribute(\"href\")  # Get the link URL\n",
    "            LinkedIn_Url.append(link_url)\n",
    "\n",
    "            try:\n",
    "                # Check for the image element\n",
    "                missing_image = li.find_element(By.CSS_SELECTOR, \"img.evi-image.lazy-image.ghost-person.ember-view\")\n",
    "                Non_existent_Image.append(i)\n",
    "            except Exception:\n",
    "                # No action needed if the image is not found, since it's already captured in the list.\n",
    "                pass\n",
    "        except Exception:\n",
    "            try:\n",
    "                # Check for the image element\n",
    "                missing_image = li.find_element(By.CSS_SELECTOR, \"img.evi-image.lazy-image.ghost-person.ember-view\")\n",
    "                Non_existent_Image.append(i)\n",
    "            except Exception:\n",
    "                # No action needed if the image is not found, since it's already captured in the list.\n",
    "                pass\n",
    "            # If the <a> element does not exist, note the position\n",
    "            Non_existent_linkedIn_Profiles.append(i)\n",
    "            \n",
    "    #print(\"Non_existent_linkedIn_Profiles\", Non_existent_linkedIn_Profiles)\n",
    "    #print(\"Non_existent_Image\", Non_existent_Image)\n",
    "    #Determeing the index of the list of pictures, where the non-available pictures are not included\n",
    "    for num in Non_existent_Image:\n",
    "        if num in Non_existent_linkedIn_Profiles:\n",
    "            #memory.append(num)\n",
    "            Non_existent_linkedIn_Profiles.remove(num)  # Remove the number from the first list\n",
    "            # Decrement numbers larger than the removed number\n",
    "    #print(\"Non_existent_linkedIn_Profiles\", Non_existent_linkedIn_Profiles)        \n",
    "    for num in reversed(Non_existent_Image):\n",
    "        Non_existent_linkedIn_Profiles = [x - 1 if x > num else x for x in Non_existent_linkedIn_Profiles]\n",
    "    #print(\"Non_existent_linkedIn_Profiles\", Non_existent_linkedIn_Profiles)\n",
    "    # Print the results without any additional output\n",
    "    return Non_existent_linkedIn_Profiles, LinkedIn_Url, len(LinkedIn_Url)\n",
    "# Example usage (assuming 'driver' is already defined and points to the correct page)\n",
    "#Non_existent_linkedIn_Profiles, LinkedIn_Url, count_existent_profile = linkedInProfiles(driver)\n",
    "#print(Non_existent_linkedIn_Profiles, LinkedIn_Url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ffd646-775c-47e4-91dd-94ff18216064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pictures(Non_existent_linkedIn_Profiles, driver):\n",
    "    ScrollAndLoad(driver)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Use BeautifulSoup to extract image URLs from the HTML source\n",
    "    from bs4 import BeautifulSoup\n",
    "    ImageUrl = []\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    images = soup.find_all('img', class_='evi-image lazy-image ember-view')\n",
    "    for img in images:\n",
    "        src = img.get('src')\n",
    "        if \"data:image/gif;base64,\" not in src:\n",
    "            ImageUrl.append(src)\n",
    "\n",
    "    # Extract Image URL for missing LinkedIn profiles\n",
    "    ImageUrl_no_linkedIn = []\n",
    "\n",
    "    # Iterate through the positions of non-existent LinkedIn profiles\n",
    "    for img in Non_existent_linkedIn_Profiles:\n",
    "        # Check if the index is within the bounds of the ImageUrl list\n",
    "        if img < len(ImageUrl):\n",
    "            ImageUrl_no_linkedIn.append(ImageUrl[img])\n",
    "        else:\n",
    "            print(f\"Index {img} is out of range for ImageUrl list and will be skipped.\")\n",
    "    \n",
    "    return ImageUrl_no_linkedIn\n",
    "\n",
    "    \"\"\"ScrollAndLoad(driver)\n",
    "    page_source = driver.page_source\n",
    "    # Use BeautifulSoup or regex to extract image URLs from the HTML source\n",
    "    from bs4 import BeautifulSoup\n",
    "    ImageUrl = []\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    images = soup.find_all('img', class_='evi-image lazy-image ember-view')\n",
    "    for img in images:\n",
    "        src = img.get('src')\n",
    "        if \"data:image/gif;base64,\" not in src:\n",
    "            ImageUrl.append(src)\n",
    "    \n",
    "    #ImageUrl = ImageUrl[1:]\n",
    "    #Extract Image URL for missing LinkedIn profiles\n",
    "    ImageUrl_no_linkedIn = []\n",
    "    # Iterate through the positions of non-existent LinkedIn profiles\n",
    "    for img in Non_existent_linkedIn_Profiles:\n",
    "        # Check if the current position is not in the list of non-existent images\n",
    "        ImageUrl_no_linkedIn.append(ImageUrl[img])\n",
    "    #print(ImageUrl_no_linkedIn)\n",
    "    return ImageUrl_no_linkedIn\n",
    "#ImageUrl_no_linkedIn = load_pictures() \n",
    "#print(ImageUrl_no_linkedIn)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46c30b48-fb56-4a6a-a944-997844a9b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare if two images are the same for testing, if the LinkedIn profile is the right one\n",
    "def compare_images(url1, url2):\n",
    "    # Fetch the images from the URLs\n",
    "    response1 = requests.get(url1)\n",
    "    response2 = requests.get(url2)\n",
    "\n",
    "    # Open the images\n",
    "    img1 = Image.open(BytesIO(response1.content))\n",
    "    img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "    # Optionally resize images to a common size for comparison\n",
    "    img1 = img1.resize((50, 50))\n",
    "    img2 = img2.resize((50, 50))\n",
    "\n",
    "    # Generate perceptual hashes\n",
    "    hash1 = imagehash.average_hash(img1)\n",
    "    hash2 = imagehash.average_hash(img2)\n",
    "\n",
    "    # Compare the hashes\n",
    "    if hash1 == hash2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fd1390e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversed_image_search(ImageUrl_no_linkedIn, driver):\n",
    "    # List of image URLs to search\n",
    "    image_urls = ImageUrl_no_linkedIn\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")  # Open in incognito mode\n",
    "    options.add_argument(\"--start-maximized\")  # Start the browser maximized\n",
    "    \n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver2 = webdriver.Chrome(options=options)\n",
    "    \n",
    "    count_google_lens = 0\n",
    "    count_google_lens_related_search = 0\n",
    "    count_not_found = 0\n",
    "    \n",
    "    # Open Google Images\n",
    "    driver2.get(\"https://images.google.com/\")\n",
    "    time.sleep(3)\n",
    "    # Wait for the page to load and the reject button to be present, then click it\n",
    "    try:\n",
    "        WebDriverWait(driver2, 10).until(EC.presence_of_element_located((By.ID, \"W0wltc\"))).click()\n",
    "    except Exception as e:\n",
    "        print(\"No reject button found or already clicked.\")\n",
    "    not_found = []#put back at the end of the code again\n",
    "    personal_profiles = []\n",
    "    for image_url in image_urls:    \n",
    "        # Open Google Images\n",
    "        driver2.get(\"https://images.google.com/\")\n",
    "    \n",
    "        # Wait for the camera icon to be clickable\n",
    "        svg_element = WebDriverWait(driver2, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"svg.Gdd5U\"))\n",
    "        )\n",
    "    \n",
    "        # Click the SVG element\n",
    "        svg_element.click()\n",
    "    \n",
    "        # Wait for the \"Paste Image URL\" input to be visible\n",
    "        paste_url_input = WebDriverWait(driver2, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"input.cB9M7\"))\n",
    "        )    \n",
    "        paste_url_input.send_keys(image_url)  # Paste the image URL\n",
    "        time.sleep(1)  # Wait a moment for any processing\n",
    "        paste_url_input.send_keys(Keys.ENTER)\n",
    "        time.sleep(3)\n",
    "    \n",
    "        #Test for Related search button, incase exact matches don't exist or don't lead anywhere\n",
    "        related_search = 0\n",
    "        try:\n",
    "            # Wait for the \"See exact matches\" to be clickable\n",
    "            svg_element = WebDriverWait(driver2, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.sZ3jUb\"))\n",
    "            )\n",
    "            related_search = 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # Open search results\n",
    "        try:\n",
    "            # Wait for the \"See exact matches\" to be clickable\n",
    "            svg_element = WebDriverWait(driver2, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.WF9wo\"))\n",
    "            )\n",
    "            \n",
    "            # Click the SVG element\n",
    "            svg_element.click()\n",
    "        except:\n",
    "            # Wait for the \"See exact matches\" to be clickable\n",
    "            svg_element = WebDriverWait(driver2, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.ICt2Q\"))\n",
    "            )\n",
    "            \n",
    "            # Click the SVG element\n",
    "            svg_element.click()\n",
    "    \n",
    "        time.sleep(1)\n",
    "    \n",
    "        def LinkedIn_urls():\n",
    "            # Get the page source after the search\n",
    "            page_source = driver2.page_source\n",
    "        \n",
    "            # Create a BeautifulSoup object with the page source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            linkedin_links = []\n",
    "            # Find all <a> tags and filter for LinkedIn links\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                if 'linkedin.com' in link['href']:\n",
    "                    linkedin_links.append(link['href'])\n",
    "        \n",
    "            # Remove duplicates by converting the list to a set, then back to a list\n",
    "            linkedin_links = list(set(linkedin_links))\n",
    "            return linkedin_links\n",
    "    \n",
    "        linkedin_links = LinkedIn_urls()\n",
    "        def is_personal_linkedin_profile(link):\n",
    "            \"\"\"Check if the given LinkedIn link is a personal profile link.\"\"\"\n",
    "            return (\"linkedin.com/in/\" in link)\n",
    "    \n",
    "        # Filter personal profile links\n",
    "        List = [link for link in linkedin_links if is_personal_linkedin_profile(link)]\n",
    "        #Identify if LinkedIn is the right LinkedIn of the respective person\n",
    "        \"\"\"\n",
    "        if List != []:\n",
    "            List2 = []\n",
    "            for link in List:\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "                driver.get(link)\n",
    "                # Wait for the image element to be visible on the page\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, 'pv-top-card-profile-picture__image--show'))\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading page for {link}: {e}\")\n",
    "                    continue  # Skip to the next link if there's an error\n",
    "                page_source = driver.page_source\n",
    "                # Use BeautifulSoup or regex to extract image URLs from the HTML source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                # Find the image element (you can use class, id, or other attributes)\n",
    "                image_element = soup.find('img', class_='pv-top-card-profile-picture__image--show')\n",
    "                \n",
    "                # Extract the 'src' attribute\n",
    "                if image_element:\n",
    "                    image_src = image_element.get('src')\n",
    "                    \n",
    "            \n",
    "                are_same = compare_images(image_src, image_url)\n",
    "                if are_same:\n",
    "                    List2.append(link)\n",
    "            List = List2\n",
    "        \"\"\"\n",
    "        #Counter for LinkedIn profiles found via Google Lens Web scraping\n",
    "        if len(List)>0:\n",
    "            count_google_lens = count_google_lens + 1\n",
    "        List = List[:3]        \n",
    "        # If the list is empty and previously tested Related search exists try Related search\n",
    "        if List == [] and related_search == 1:\n",
    "            link = \"\"\n",
    "            try:\n",
    "                \"\"\"# Wait for the \"See exact matches\" to be clickable\n",
    "                svg_element = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.sZ3jUb\"))\n",
    "                )\n",
    "                svg_element.click()\"\"\"\n",
    "                page_source = driver2.page_source\n",
    "                # Create a BeautifulSoup object\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                \n",
    "                # Use a CSS selector to find the link\n",
    "                link = soup.select_one('a[href^=\"https://www.google.com/search?q=\"]')\n",
    "                \n",
    "            except:\n",
    "                try:\n",
    "                    driver2.back()\n",
    "                    \"\"\"# Wait for the \"See exact matches\" to be clickable\n",
    "                    svg_element = WebDriverWait(driver, 5).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.sZ3jUb\"))\n",
    "                    )\n",
    "                    svg_element.click()\"\"\"\n",
    "                    page_source = driver2.page_source\n",
    "                    # Create a BeautifulSoup object\n",
    "                    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                    \n",
    "                    # Use a CSS selector to find the link\n",
    "                    link = soup.select_one('a[href^=\"https://www.google.com/search?q=\"]')\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "            # Extract the href attribute if link successfully found\n",
    "            if link != \"\":\n",
    "                try:\n",
    "                    extracted_link = link['href']\n",
    "                    driver2.get(extracted_link)\n",
    "                    linkedin_links = LinkedIn_urls()\n",
    "                    # Filter personal profile links\n",
    "                    List = [link for link in linkedin_links if is_personal_linkedin_profile(link)]\n",
    "                except:\n",
    "                    pass\n",
    "            if List != []:\n",
    "                count_google_lens_related_search = count_google_lens_related_search +1\n",
    "    \n",
    "        if List == []:\n",
    "            count_not_found = count_not_found + 1\n",
    "        personal_profiles.extend(List)\n",
    "        # Get links that are not personal profiles\n",
    "        non_personal_links = [link for link in linkedin_links if link not in personal_profiles]\n",
    "        \n",
    "        \"\"\"if len(final_linkedin_links)<1:\n",
    "            for image in not_found:\n",
    "                list = load_all_images(image)\n",
    "                images.extend(list)\n",
    "        \"\"\"\n",
    "    # Output the LinkedIn links found\n",
    "    return personal_profiles, count_google_lens, count_google_lens_related_search, count_not_found\n",
    "    \n",
    "#personal_profiles, count_google_lens, count_google_lens_related_search, count_not_found = reversed_image_search(ImageUrl_no_linkedIn)\n",
    "#print (personal_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814258a-b59c-4f30-84cb-3f6c3d8e62fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09aa8879-694f-4c23-8b5d-532bd5efa582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def linkedInProfiles():\\n    from selenium import webdriver\\n    from selenium.webdriver.common.by import By\\n    from selenium.webdriver.chrome.service import Service\\n    from selenium.webdriver.chrome.options import Options\\n    import time\\n    \\n    # Find the parent <ul> element by its class name\\n    ul_element = driver.find_element(By.CSS_SELECTOR, \"ul.display-flex.list-style-none.flex-wrap\")\\n    \\n    # Find all <li> elements within the <ul>\\n    li_elements = ul_element.find_elements(By.CSS_SELECTOR, \"li.grid.grid__col--lg-8.block.org-people-profile-card__profile-card-spacing\")\\n    \\n    for li in li_elements:\\n        # Check for the <a> element with the specified class name\\n        try:\\n            link_element = li.find_element(By.CSS_SELECTOR, \"a.app-aware-link.link-without-visited-state\")\\n            link_url = link_element.get_attribute(\"href\")  # Get the link URL\\n            print(\"Opening link:\", link_url)\\n            #driver.get(link_url)  # Open the link\\n            #time.sleep(5)  # Wait for the new page to load\\n            #driver.back()  # Go back to the previous page\\n            time.sleep(2)  # Wait for the page to reload\\n        except Exception as e:\\n            # If the <a> element does not exist, copy the image source\\n            img_element = li.find_element(By.CSS_SELECTOR, \"img.evi-image.lazy-image.ghost-default.ember-view.org-people-profile-card__cover-photo.org-people-profile-card__cover-photo--people\")\\n            img_src = img_element.get_attribute(\"src\")  # Get the image source\\n            print(\"Image source:\", img_src)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check on LinkedIn if it is a personal profile: looking at the link is slightly less reliable but way faster\n",
    "\"\"\"def linkedInProfiles():\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import time\n",
    "    \n",
    "    # Find the parent <ul> element by its class name\n",
    "    ul_element = driver.find_element(By.CSS_SELECTOR, \"ul.display-flex.list-style-none.flex-wrap\")\n",
    "    \n",
    "    # Find all <li> elements within the <ul>\n",
    "    li_elements = ul_element.find_elements(By.CSS_SELECTOR, \"li.grid.grid__col--lg-8.block.org-people-profile-card__profile-card-spacing\")\n",
    "    \n",
    "    for li in li_elements:\n",
    "        # Check for the <a> element with the specified class name\n",
    "        try:\n",
    "            link_element = li.find_element(By.CSS_SELECTOR, \"a.app-aware-link.link-without-visited-state\")\n",
    "            link_url = link_element.get_attribute(\"href\")  # Get the link URL\n",
    "            print(\"Opening link:\", link_url)\n",
    "            #driver.get(link_url)  # Open the link\n",
    "            #time.sleep(5)  # Wait for the new page to load\n",
    "            #driver.back()  # Go back to the previous page\n",
    "            time.sleep(2)  # Wait for the page to reload\n",
    "        except Exception as e:\n",
    "            # If the <a> element does not exist, copy the image source\n",
    "            img_element = li.find_element(By.CSS_SELECTOR, \"img.evi-image.lazy-image.ghost-default.ember-view.org-people-profile-card__cover-photo.org-people-profile-card__cover-photo--people\")\n",
    "            img_src = img_element.get_attribute(\"src\")  # Get the image source\n",
    "            print(\"Image source:\", img_src)\"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cffe97-d973-4cb2-8be7-84f89edb7e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bad715d5-029a-4276-9fb6-2913b05cbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Put data in dataframe\n",
    "def Connect(company_name, counts_df, linkedin_links_df, driver):\n",
    "    #Call Connector Functions and put the results in a DataFrame\n",
    "    # Execute Connector methods\n",
    "    ScrollAndLoad(driver)\n",
    "    \n",
    "    # Get LinkedIn profiles and counts\n",
    "    Non_existent_linkedIn_Profiles, LinkedIn_Url, count_existent_profile = linkedInProfiles(driver)\n",
    "    #print(Non_existent_linkedIn_Profiles, LinkedIn_Url)\n",
    "\n",
    "    # Load pictures for non-existent profiles\n",
    "    ImageUrl_no_linkedIn = load_pictures(Non_existent_linkedIn_Profiles, driver)\n",
    "    \n",
    "    # Perform reverse image search\n",
    "    personal_profiles, count_google_lens, count_google_lens_related_search, count_not_found = reversed_image_search(ImageUrl_no_linkedIn, driver)\n",
    "    #print(personal_profiles)\n",
    "    \n",
    "    # Prepare the total counts\n",
    "    count_total = count_google_lens + count_google_lens_related_search + count_not_found + count_existent_profile\n",
    "    count_linkedin = count_google_lens + count_google_lens_related_search + count_existent_profile\n",
    "    \n",
    "    # Prepare data for the LinkedIn links DataFrame\n",
    "    for link in LinkedIn_Url:  # Links from LinkedIn_Url\n",
    "        linkedin_links_df = pd.concat([linkedin_links_df, \n",
    "                                       pd.DataFrame({\"Company\": [company_name], \n",
    "                                                      \"LinkedIn Link\": [link], \n",
    "                                                      \"Source\": [0]})], ignore_index=True)\n",
    "    \n",
    "    for link in personal_profiles:  # Links from personal_profiles\n",
    "        linkedin_links_df = pd.concat([linkedin_links_df, \n",
    "                                       pd.DataFrame({\"Company\": [company_name], \n",
    "                                                      \"LinkedIn Link\": [link], \n",
    "                                                      \"Source\": [1]})], ignore_index=True)\n",
    "    \n",
    "    # Prepare data for the counts DataFrame\n",
    "    temp_counts_df = pd.DataFrame({\n",
    "        \"Company\": [company_name],\n",
    "        \"Total Count\": [count_total],\n",
    "        \"Google Lens Count\": [count_google_lens],\n",
    "        \"Google Lens Related Search Count\": [count_google_lens_related_search],\n",
    "        \"Not Found Count\": [count_not_found],\n",
    "        \"Existent Profile Count\": [count_existent_profile]\n",
    "    })\n",
    "    \n",
    "    # Concatenate the temporary counts DataFrame with the main counts DataFrame\n",
    "    counts_df = pd.concat([counts_df, temp_counts_df], ignore_index=True)\n",
    "\n",
    "    return counts_df, linkedin_links_df\n",
    "    # After the loop, you can print or return the DataFrames\n",
    "    #print(linkedin_links_df)\n",
    "    #print(counts_df\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b72a7c-6730-4d21-8b90-bf637d28f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def Connect(internal_id, company_name, counts_df, linkedin_links_df, driver):\n",
    "    try:\n",
    "        # Execute Connector methods\n",
    "        ScrollAndLoad(driver)\n",
    "    \n",
    "        # Get LinkedIn profiles and counts\n",
    "        Non_existent_linkedIn_Profiles, LinkedIn_Url, count_existent_profile = linkedInProfiles(driver)\n",
    "    \n",
    "        # Load pictures for non-existent profiles\n",
    "        ImageUrl_no_linkedIn = load_pictures(Non_existent_linkedIn_Profiles, driver)\n",
    "    \n",
    "        # Perform reverse image search\n",
    "        personal_profiles, count_google_lens, count_google_lens_related_search, count_not_found = reversed_image_search(ImageUrl_no_linkedIn, driver)\n",
    "    \n",
    "        # Prepare the total counts\n",
    "        count_total = count_google_lens + count_google_lens_related_search + count_not_found + count_existent_profile\n",
    "        \n",
    "        # Collect LinkedIn links data\n",
    "        linkedin_data = []\n",
    "        for link in LinkedIn_Url:  # Links from LinkedIn_Url\n",
    "            linkedin_data.append({\"internal_id\": internal_id, \"Company\": company_name, \"LinkedIn Link\": link, \"Source\": 0})\n",
    "        \n",
    "        for link in personal_profiles:  # Links from personal_profiles\n",
    "            linkedin_data.append({\"internal_id\": internal_id, \"Company\": company_name, \"LinkedIn Link\": link, \"Source\": 1})\n",
    "        \n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        if linkedin_data:\n",
    "            temp_linkedin_df = pd.DataFrame(linkedin_data)\n",
    "            linkedin_links_df = pd.concat([linkedin_links_df, temp_linkedin_df], ignore_index=True)\n",
    "    \n",
    "        # Prepare data for the counts DataFrame\n",
    "        temp_counts_df = pd.DataFrame({\n",
    "            \"internal_id\": [internal_id], \n",
    "            \"Company\": [company_name],\n",
    "            \"Total Count\": [count_total],\n",
    "            \"Google Lens Count\": [count_google_lens],\n",
    "            \"Google Lens Related Search Count\": [count_google_lens_related_search],\n",
    "            \"Not Found Count\": [count_not_found],\n",
    "            \"Existent Profile Count\": [count_existent_profile]\n",
    "        })\n",
    "        \n",
    "        # Concatenate the temporary counts DataFrame with the main counts DataFrame\n",
    "        counts_df = pd.concat([counts_df, temp_counts_df], ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "    return counts_df, linkedin_links_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6fd1e7-9ee5-438b-96b5-0a885787314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Founder\n",
    "def Founder_Filter(driver):\n",
    "    try:\n",
    "        searchinput = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID,\"people-search-keywords\"))\n",
    "        )\n",
    "        searchinput.clear()#making sure that there is not text inside because the sended key will be appended\n",
    "        searchinput.send_keys(\"founder - founder's associate\")\n",
    "        searchinput.send_keys(Keys.ENTER)\n",
    "    except Exception as e:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280145a2-dc56-442f-ac31-5ad39e5df357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
